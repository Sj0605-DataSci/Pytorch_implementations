# Pytorch_implementations

### Transformer Implementation from Scratch
This repository contains an implementation of the Transformer model introduced in the paper "Attention is All You Need" by Vaswani et al., implemented from scratch using Python and PyTorch. This project was undertaken as a practice exercise to deepen understanding of the Transformer architecture and its components.
The Transformer model, introduced in the paper by Vaswani et al., revolutionized the field of natural language processing by replacing recurrent and convolutional layers with self-attention mechanisms. It achieved state-of-the-art results on various machine translation tasks, outperforming traditional sequence-to-sequence models.

Dependencies
To run this code, you will need:

Python (>=3.6)
PyTorch (>=1.8.0)

References
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).


### LoRA Implementation from Scratch
This repository contains an implementation of LoRA for practice

Dependencies
To run this code, you will need:

Python (>=3.6)
PyTorch (>=1.8.0)

### Unconditional Diffuser - Image Generation from Hf course
This repository contains an implementation of an unconditional stable diffuser for butterfly generation

Dependencies
To run this code, you will need:

Python (>=3.6)
PyTorch (>=1.8.0), diffusers, pyarrow and etc mentioned in ipynb file
